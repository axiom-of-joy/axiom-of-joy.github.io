---
layout: post
title: "PageRank Beyond the Web"
subtitle: "Ranking March Madness teams with Google's famous algorithm."
author: "Alexander Song"
background: '/media/posts/mbo_part1/network.jpg'
extra_css: '/assets/post_css/pagerank.css'
---
\\(\\newcommand{\\pagerank}{\\mathrm{PageRank}}\\newcommand\\given[1][]{\\:#1\\vert\\:}\\newcommand{\\diag}{\\mathrm{diag}}\\)

# PageRank Beyond the Web

This post provides an introduction to PageRank, Google's algorithm for ranking web pages based upon their link structure. In a nutshell, the algorithm models the behavior of an internet surfer who navigates the web by randomly selecting hyperlinks and occasionally entering URLs. While best known as a method for ranking web sites, PageRank has found a variety of applications beyond the web, from FIXME to FIXME. I'll first introduce the mathematics behind the PageRank algorithm, including a discussion of random walks, Markov chains, and the Perron-Frobenius Theorem. I'll then use PageRank to produce a ranking of NCAA basketball teams by constructing a so-called "winner network" that models the behavior of a prototypical "bandwagon" fan who frequently shifts his allegiance to follow the best teams.

## The Mathematics of PageRank

I'll introduce the PageRank algorithm in its original context of ranking web pages. Suppose the internet is represented as an unweighted directed graph \\(G = (V, E)\\), where the vertices of \\(V\\) represent web pages and the directed edges of \\(E\\) represent hyperlinks.

<figure>
        <div class="caption">Figure 1: A representation of the internet as an unweighted directed graph. Directed edges indicate hyperlinks.</div>
    <div class="row">
        <div class="column">
            <img src="/media/posts/pagerank/web.png" alt="A graph representing an internet of four web sites." class="center1">
        </div>
        <div class="column">
          \[
            A=
              \begin{pmatrix}
                0 & 1 & 1 & 1\\
                0 & 0 & 1 & 1\\
                0 & 0 & 0 & 1\\
                0 & 0 & 1 & 0
              \end{pmatrix}
          \]
        </div>
    </div>
    <div class="row">
      <div class="column">
        <div class="caption">(a) A graph representing the web.</div>
      </div>
      <div class="column">
        <div class="caption">(b) The adjacency matrix for the graph.</div>
      </div>
    </div>
</figure>

 A random walk on \\(G\\) then models the behavior of a surfer who navigates the web by following hyperlinks uniformly at random and occasionally typing in URLs uniformly at random. The Perron-Frobenius Theorem guarantees that such a random walk converges to a unique "stationary distribution," a probability mass function \\(\\boldsymbol \\pi\\) on the vertex set \\(V\\) that remains unchanged at each step of the random walk. The PageRank of a web page is then just the probability that our idealized surfer visits said web page, according to the stationary distribution \\(\\boldsymbol\\pi\\). Web pages with the greatest likelihood of being visited are deemed the most important and are ranked accordingly. I'll start by introducing the theory of random walks and Markov matrices.

### A Motivating Example

Let \\(G = (V, E)\\) with \\(\\lvert V \\rvert = n\\) denote the unweighted directed graph described above. The _adjacency matrix_ \\(A \\in \\mathbb R^{n\\times n}\\) for \\(G\\) is defined by
\\[
a_{ij} =
\\begin{cases}
1 &\\mbox{if \\((i,j) \\in E\\),}\\\
0 &\\mbox{otherwise}
\\end{cases}
\\]
for \\(1 \\leq i \\leq n\\). That is, \\(a_{ij}\\) is 1 if there is a hyperlink from web page \\(i\\) to web page \\(j\\) and 0 otherwise. We define the _outdegree_ of a vertex \\(i \in V\\) by
\\[
r_i = \\sum_{j=1}^n a_{ij},
\\]
so that \\(r_i\\) denotes the number of hyperlinks from webpage \\(i\\) to other pages. A _random walk_ on \\(G\\) is a sequence of vertices \\((v_t)\_{t \\geq 0}\\) chosen iteratively so that
\\begin{equation} \\label{eq:iterative_pagerank}
P(v_{t+1} = j \\given v_t = i) = a_{ij} / r_i
\\end{equation}
for all \\(i,j\\in V\\). Intuitively, we can imagine an internet user starting at some initial webpage and surfing the web by clicking on hyperlinks uniformly at random. This suggests a pitfall in our definition, since the graph \\(G\\) may contain vertices with no outgoing edges corresponding to web pages with no hyperlinks. It is thus possible for our internet surfer to "get stuck" on a so-called "dangling node." One method to avoid this pitfall is to introduce a so-called "transportation parameter" \\(\\alpha\\) satisfying \\(0 < \\alpha < 1\\) to represent the probability that our internet surfer follows a hyperlink at any given time step (in practice, \\(\\alpha\\) is chosen to be close to 1 so that our surfer almost always follows a hyperlink). However, with probability \\(1 - \\alpha\\), the surfer will type in a URL and "teleport" to a (possibly non-adjacent) vertex chosen uniformly at random from the vertex set \\(V\\). The probability that an internet user at web page \\(i\\) at time \\(t\\) will visit web page \\(j\\) at time \\(t + 1\\) is thus given by

\\begin{equation} \\label{eq:pagerank_transition_probability}
P(v_{t+1} = j \\given v_t = i) =
\\begin{cases}
\\alpha\\left(\\frac{a_{ij}}{r_i}\\right) + \\frac{1-\\alpha}{n} & \\mbox{if \\(r_i \\neq 0\\),} \\\
\\frac 1n & \\mbox{otherwise.}
\\end{cases}
\\end{equation}

The above equation is equivalent to the most common formulation of PageRank, a _strongly preferential_ PageRank with uniform teleportation distribution. Later in this post, I'll introduce a more general framework for understanding PageRank and will explain a few less common versions of the algorithm.

### Markov Chains and the Perron-Frobenius Theorem

Random walks are typically described using _Markov matrices_ (also known as _stochastic matrices_), real square matrices with nonnegative entries such that each row sums to 1. The entry \\(m_{ij}\\) of a Markov matrix \\(M\\) indicates the probability that a random walker located at vertex \\(i\\) at time \\(t\\) moves to vertex \\(j\\) at time \\(t + 1\\). A probability distribution on the vertex set \\(V\\) may be represented as a _stochastic vector_, a row vector with real, nonnegative entries that sum to 1. If \\(M \\in \\mathbb R^{n \\times n}\\) is a Markov matrix for a random walk on the graph \\(G\\) and \\(\\mathbf p^0 \\in \\mathbb R^{1 \\times n}\\) is a stochastic vector such that \\(\\mathbf p^0\_i\\) indicates the probability that the random walk starts at vertex \\(i\\), you can verify that \\(\\mathbf p^1 = \\mathbf p^0M\\) is a stochastic vector in which \\(\\mathbf p^1\_j\\) denotes the probability that the random walker is located at vertex \\(j\\) after one time step. By induction, \\(\\mathbf p^s = \\mathbf p^0M^s\\) is then a stochastic vector in which \\(\\mathbf p^s_i\\) denotes the probability that the random walker is located at vertex \\(i\\) after \\(s\\) time steps.

We can conveniently represent the random walk of our internet surfer by the Markov matrix \\(M \\in \\mathbb R^{n \\times n}\\), where \\(m_{ij}\\) denotes the probability defined in \\eqref{eq:pagerank_transition_probability}. In virtue of the positive teleportation parameter \\(\\alpha\\), this matrix has the special property of being a _positive_ matrix (i.e., \\(m_{ij} > 0\\) for \\(1 \\leq i, j \\leq n\\)). \\(M\\) thus satisfies the conditions of the Perron-Frobenius Theorem as presented below. I must introduce a few definitions to state this theorem precisely. First, a real vector is said to be _positive_ if each of its entries is positive. Second, an eigenvalue \\(\\lambda_i\\) associated to a matrix \\(A \\in \\mathbb C^{n \\times n}\\) is said to be _dominant_ if \\(\\lvert \\lambda_i \\rvert > \\lvert \\lambda_j \\rvert\\) for \\(1 \\leq j \\leq n, j \\neq i\\). The eigenvector of \\(A\\) corresponding to \\(\\lambda\\) is called the _dominant eigenvector_ of \\(A\\).

<div class="theorem" text='Perron-Frobenius'>
If \(P \in \mathbb R^{n \times n}\) is a positive matrix, then \(P\) has a dominant eigenvalue \(\lambda\) with algebraic multiplicity 1. Moreover, there exist a positive right eigenvector \(\mathbf v\) and a positive left eigenvector \(\mathbf w\) of \(P\) corresponding to \(\lambda\), which are respectively the unique positive right and left eigenvectors of \(A\) (up to multiplication by a positive constant), and
\[
\lim_{k\rightarrow \infty} \frac{P^k}{\lambda^k} = \mathbf v \mathbf w.
\]
</div>

Since \\(M\\) is a Markov matrix, \\(M\\mathbf 1 = \\mathbf 1\\), where \\(\\mathbf 1 \\in \\mathbb R^n\\) denotes the _one-vector_ whose entries are all 1. Now \\(M\\) is a positive matrix and \\(\\mathbf 1\\) is a positive right eigenvector of \\(M\\) corresponding to \\(\\lambda = 1\\), so the Perron-Frobenius Theorem implies that \\(1\\) is the dominant eigenvalue of \\(M\\) and that there exists a unique positive left eigenvector \\(\\boldsymbol \\pi\\) of \\(M\\) corresponding to \\(\\lambda = 1\\) such that \\(\\lVert \\boldsymbol \\pi \\rVert_1 = 1\\). \\(\\boldsymbol \\pi\\) is the unique stochastic vector satisfying \\(\\boldsymbol \\pi M = \\boldsymbol \\pi\\), and is thus known as the _stationary distribution_ associated to the Markov matrix \\(M\\), since it remains unchanged at each time step of the random walk. By the final statement of the Perron-Frobenius Theorem,
\\begin{equation} \\label{eq:power_method}
\\lim_{k\\rightarrow \\infty} M^k = \\mathbf 1 \\boldsymbol \\pi.
\\end{equation}
For any stochastic vector \\(\\mathbf p_0 \\in \\mathbb R^{1 \\times n}\\) representing an initial probability distribution on the vertex set \\(V\\), we can left-multiply the above equation by \\(\\mathbf p_0\\) to obtain
\\[
\\lim_{k\\rightarrow\\infty} \\mathbf p_0M^k = \\boldsymbol \\pi.
\\]
Intuitively, this equation states that any initial probability distribution converges to the stationary distribution \\(\\boldsymbol \\pi\\). We then define the PageRank as a real-valued function on the vertex set \\(V\\),
\\[
\\pagerank(i) = \\boldsymbol \\pi_i, \\quad i \\in V.
\\]
In other words, the PageRank of a webpage \\(i\\) is the probability that a surfer visits the webpage \\(i\\), according to the stationary distribution \\(\\boldsymbol \\pi\\). Pages for which this probability is high are deemed important, and a ranking of the relative importance of all pages on the web may be obtained by sorting the entries of \\(\\boldsymbol \\pi\\).

### A More General Framework

Now that I have motivated PageRank in the context of the web and introduced the concepts of random walks and Markov chains, I am going to present a more general framework that will allow me to discuss variants on the the standard PageRank algorithm (this presentation closely follows FIXME). Suppose we start with a directed graph \\(G\\) and try to construct a Markov matrix for a random walk on the graph. However, we reach a dangling node and are dismayed to find that an entire row of our matrix \\(\\overline P\\) is zero. This failed stochastic matrix is an example of a _substochastic_ matrix, a square matrix with nonnegative entries and rows that sum to _at most_ 1. Now \\(\\overline P\\) cannot have full rank (since it has one or more rows of zeros), so
\\[
\\mathbf x \\overline P = \\mathbf x
\\]
cannot have a unique solution, and the stationary distribution is thus ill-defined. To remedy this issue, we fix all the dangling nodes by adding outgoing edges, or equivalently, fix all of the rows of zeros in \\(\\overline P\\) to obtain a stochastic matrix \\(P\\). But we still have a problem; while \\(P\\) is now a stochastic matrix and the random walk on \\(G\\) is now well-defined, \\(P\\) may still fail to have full rank or may not have a strictly dominant eigenvalue, and so the stationary distribution may still be ill-defined. Our solution is to construct the positive Markov matrix \\(M = \\alpha P + (1 - \\alpha) \\mathbf 1 \\mathbf v\\), where \\(0 < \\alpha < 1\\) is the aforementioned "teleportation parameter" and \\(\\mathbf v \\in \\mathbb R^{1 \\times n}\\) is a stochastic vector known as the "teleportation distribution." The _core PageRank problem_ is then the eigenvalue problem
\\[
\\mathbf x M = \\mathbf x \\qquad \\mbox{subject to} \\quad \\lVert \\mathbf x \\rVert_1 = 1, \\quad \\mathbf x > 0,
\\]
which is guaranteed to have a unique solution by the Perron-Frobenius Theorem. The three versions of PageRank I will discuss, known as _strongly preferential_, _weakly preferential_, and _sink preferential_, follow this general framework, but differ in how the stochastic matrix \\(P\\) is obtained from the substochastic matrix \\(\\overline P\\).

The strongly preferential rendition of PageRank is the most common and is sometimes referred to simply as _the_ PageRank algorithm. Let \\(\\d\\in\\mathbb R^n\\), where \\(\\mathbf d_i = 1\\) if the i<sup>th</sup> row of \\(\\overline P\\) contains only zeros, and \\(\\mathbf d_i = 0\\). Strongly preferential PageRank starts with the substochastic matrix \\(\\overline P\\) and then constructs \\(P_{\\mathbf v} := \\overline P + \\mathbf d\\mathbf v\\), where \\(\\mathbf v \\in \\mathbb R^{1 \\times n}\\) is importantly the teleportation distribution from FIXME. In other words, \\(P_s\\) is obtained by replacing all rows of zeros in \\(\\overline P\\) by the teleportation distribution \\(\\mathbf v\\), allowing a random walker to escape from any dangling node. You can verify that if \\(\\mathbf v\\) is chosen to be the uniform distribution (i.e., \\((1 / n)\\mathbf 1^T\\)), then the Markov matrix \\(M = \\alpha P_{\\mathbf v} + (1 - \\alpha) \\mathbf 1 \\mathbf v\\) is equivalent to \eqref{eq:pagerank_transition_probability}.

Weakly preferential PageRank is virtually identical to strongly preferential PageRank, except a different distribution vector is added to the rows of zeros in \\(\\overline P\\), i.e., \\(P_{\\mathbf u} := \\overline P + \\mathbf 1 \\mathbf u\\) for some stochastic vector \\(\\mathbf u\\) that is independent of \\(\\mathbf v\\). This rendition of PageRank might be preferable if we have some intuition of how our random walker should behave when he encounters a dangling node. The last version of PageRank, known as _sink preferential_ PageRank, is perhaps the simplest. In this version, a random walker simply stays put when he encounters a dangling node. Mathematically, a self-loop is added to each dangling node and the stochastic matrix for the random walk is defined as \\(P_{\\mathbf d} = \\overline P + \\diag(\\mathbf d)\\). As I'll now explain, this last version of PageRank is best suited for the task of ranking NCAA tournament teams.

## March Madness Power Ranking

Each year, 32 college teams automatically qualify for the NCAA men's basketball tournament, and an additional 36 teams are selected by a committee of experts. All 68 teams are then ranked in a seeding process that determines 60 of the 64 slots in the NCAA bracket, with the eight lowest-seeded teams competing in four games to qualify for the remaining four slots. Let's take a moment to appreciate the difficulty of accurately and fairly ranking these teams. As of this writing, there are 351 Division I men's basketball programs, but a typical regular season consists of only about 30 games. It is thus a mathematical certainty that the vast majority of teams will never face each other during the course of a single regular season. Moreover, strength of schedule can vary widely between different conferences. For example, the Atlantic Coast Conference (ACC), which sent nine teams to the 2018 NCAA tournament, is vastly more competitive than the Pac-12, which sent only three. Thus, a naive comparison of records, point differentials, or other in-game statistics will inevitably penalize worthy teams who simply play against stiffer competition. As we shall see, the PageRank algorithm is well-suited to precisely this type of problem, since it was designed for sparse graphs and automatically adjusts for strength of schedule.

### The Winner Network

Applying PageRank supposes we have constructed some sort of graph to represent our data. In the current context, we'll construct a so-called "winner network," a graph whose vertices represent teams and whose edges describe the flow of "prestige" from one team to another. After each game, a directed edge from the loser to the winner will be added to the graph and weighted by the margin of victory (in contrast to the unweighted graph representing the internet). If two teams face each other a second time, we simply add the margin of victory to the appropriate edge.

<figure>
  <img src="/media/posts/pagerank/teams.png" alt="Winner Network" class="center1">
  <div class="caption">Figure 2: A winner network in which Duke lost to Kentucky by 10, Kentucky lost to Duke by 2, Duke lost to Kansas by 7, and Kentucky lost to Kansas by 21.</div>
</figure>

If \\(w_{ij}\\) denotes the weight of the edge from \\(i\\) to \\(j\\), the _outdegree_ of node \\(i\\) in the weighted graph \\(G\\) is the same as before (replacing \\(a_{ij}\\) by \\(w_{ij}\\)), and a _random walk_ on \\(G\\) is an iteratively chosen sequence of vertices satisfying \\eqref{eq:iterative_pagerank} (replacing \\(a_{ij}\\) by \\(w_{ij}\\)). A random walk on the winner network has the attractive interpretation of a bandwagon fan who randomly changes his loyalty based upon the fortunes of his current team. While FIXME uses the winner network and strongly preferential PageRank to rank college football teams, I elect to use the sink preferential variant. The sink preferential construction has an intuitive justification in the current context, since dangling nodes in the winner network represent undefeated teams that a prototypical bandwagon would be unlikely to abandon FIXME. In keeping with FIXME, I use a teleportation parameter of \\(\\alpha = 0.85\\) in the following implementation.

### Implementation


Let's load the data set and display the first ten Duke games of the 2017-18 regular season.


```python
import pandas as pd
import numpy as np
import scipy.sparse.linalg as splin
from IPython.display import display, HTML

# Load DataFrame.
path = 'data/2017-18/'
file = 'scores.csv'
df = pd.read_csv(path + file, index_col=0, parse_dates=['Date'])

# Display first ten Duke games.
team = 'duke'
ind = (df['Team1']==team) | (df['Team2']==team)
display(HTML(df.loc[ind].head(5).to_html(index=False)))
```

<div align="middle" class="w3-responsive">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Date</th>
      <th>GameType</th>
      <th>Team1</th>
      <th>Pts1</th>
      <th>Team2</th>
      <th>Pts2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2017-11-10</td>
      <td>REG</td>
      <td>duke</td>
      <td>97</td>
      <td>elon</td>
      <td>68</td>
    </tr>
    <tr>
      <td>2017-11-11</td>
      <td>REG</td>
      <td>duke</td>
      <td>99</td>
      <td>utah-valley</td>
      <td>69</td>
    </tr>
    <tr>
      <td>2017-11-14</td>
      <td>REG</td>
      <td>duke</td>
      <td>88</td>
      <td>michigan-state</td>
      <td>81</td>
    </tr>
    <tr>
      <td>2017-11-17</td>
      <td>REG</td>
      <td>duke</td>
      <td>78</td>
      <td>southern</td>
      <td>61</td>
    </tr>
    <tr>
      <td>2017-11-20</td>
      <td>REG</td>
      <td>duke</td>
      <td>92</td>
      <td>furman</td>
      <td>63</td>
    </tr>
  </tbody>
</table>
</div>

Some statistics will give us a better idea of the contents of our data set.

```python
# List of teams.
teams = df['Team1'].values.tolist() + df['Team2'].values.tolist()
teams = list(set(teams))
teams.sort()
n = len(teams)

# Pre-tournament DataFrame.
pre_df = df[df['Date'] < '2018-03-13']
pre_df = pre_df.drop(['Date', 'GameType'], axis=1)

# NCAA tournament DataFrame.
tourn_df = df[df['GameType']=='NCAA']
tourn_df = tourn_df.drop(['GameType'], axis=1)

# Display stats.
index = [ 'Teams', 'Total Games', 'Pre-tournament Games', 'Tournament Games' ]
data = [ n, len(df.index), len(pre_df.index), len(tourn_df.index) ]
columns = [ 'Category', 'Count']
stats_df = pd.DataFrame(list(zip(index, data)), columns=columns)
stats_df = stats_df.set_index('Category')
display(stats_df)
```

<div align="middle" class="w3-responsive">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Count</th>
    </tr>
    <tr>
      <th>Category</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Teams</th>
      <td>656</td>
    </tr>
    <tr>
      <th>Total Games</th>
      <td>6004</td>
    </tr>
    <tr>
      <th>Pre-tournament Games</th>
      <td>5874</td>
    </tr>
    <tr>
      <th>Tournament Games</th>
      <td>67</td>
    </tr>
  </tbody>
</table>
</div>


Our data set contains 6004 games played between 656 NCAA teams from Division I, II, and III in the 2017-18 season. The NCAA tournament itself consists of 67 games between 68 teams: four qualification games and 63 elimination games in a bracket of 64 teams. I'll use sink preferential PageRank to rank all 656 teams based upon their performance prior to the start of the 2018 NCAA men's basketball tournament on March 13. I'll then use the outcome of the tournament games to evaluate PageRank relative to other rankings.

The trickiest part of implementing PageRank is constructing the positive Markov matrix \\(M\\).


```python
# Create a dictionary mapping team names to indices.
team_dict = dict(zip(teams, range(n)))

# Sub-stochastic matrix P_bar.
P_bar = np.zeros((n, n))

# Add an edge from loser to winner with margin of victory as edge weight.
for _, row in pre_df.iterrows():
    team1, pts1, team2, pts2 = row.values.tolist()
    sorted_tuples = sorted([(team1, pts1), (team2, pts2)], key=lambda tuple: tuple[1])
    i, j = [ team_dict[tuple[0]] for tuple in sorted_tuples ]
    P_bar[i,j] += abs(pts1 - pts2)

# Make nonzero rows sum to 1. Now P_bar is sub-stochastic.
row_sum = P_bar.sum(axis=1, dtype='float64')
d = row_sum==0
temp_ind = ~d
inv_row_sum = np.zeros(row_sum.shape)
inv_row_sum[temp_ind] = np.reciprocal(row_sum[temp_ind])
P_bar = P_bar * inv_row_sum[:, np.newaxis]

# Stochastic Markov matrix P for sink preferential PageRank.
P = P_bar + np.diag(d)

# Transportation parameter alpha.
alpha = 0.85

# Positive Markov matrix M.
M = alpha * P + (1 - alpha) * (1 / n)
```

Now that I've constructed \\(M\\), the next step is find the dominant eigenvector and stationary distribution \\(\\boldsymbol\\pi\\).


```python
# Solve for dominant eigenvector of M and normalize.
_, pi = splin.eigs(M.T, k=1)
pi = np.real(pi.squeeze())
pi = pi / np.sum(pi)

# Display most highly ranked teams.
tuples = zip(teams, pi.tolist())
tuples = sorted(tuples, key=lambda tuple: tuple[1], reverse=True)
columns = [ 'Team', 'PageRank' ]
pr_df = pd.DataFrame(tuples, columns=columns)
pr_df = pr_df.set_index('Team')
display(pr_df.head(25))
```


<div align="middle" class="w3-responsive" style="overflow:auto; height:400px;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PageRank</th>
    </tr>
    <tr>
      <th>Team</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>villanova</th>
      <td>0.024310</td>
    </tr>
    <tr>
      <th>west-virginia</th>
      <td>0.021227</td>
    </tr>
    <tr>
      <th>kansas</th>
      <td>0.015281</td>
    </tr>
    <tr>
      <th>virginia</th>
      <td>0.013761</td>
    </tr>
    <tr>
      <th>butler</th>
      <td>0.013637</td>
    </tr>
    <tr>
      <th>north-carolina</th>
      <td>0.013312</td>
    </tr>
    <tr>
      <th>alabama</th>
      <td>0.013008</td>
    </tr>
    <tr>
      <th>xavier</th>
      <td>0.012924</td>
    </tr>
    <tr>
      <th>florida</th>
      <td>0.012778</td>
    </tr>
    <tr>
      <th>michigan</th>
      <td>0.012000</td>
    </tr>
    <tr>
      <th>duke</th>
      <td>0.011624</td>
    </tr>
    <tr>
      <th>creighton</th>
      <td>0.011369</td>
    </tr>
    <tr>
      <th>texas-am</th>
      <td>0.011353</td>
    </tr>
    <tr>
      <th>providence</th>
      <td>0.011131</td>
    </tr>
    <tr>
      <th>texas-tech</th>
      <td>0.010611</td>
    </tr>
    <tr>
      <th>kentucky</th>
      <td>0.010300</td>
    </tr>
    <tr>
      <th>auburn</th>
      <td>0.009923</td>
    </tr>
    <tr>
      <th>houston</th>
      <td>0.009698</td>
    </tr>
    <tr>
      <th>tennessee</th>
      <td>0.009612</td>
    </tr>
    <tr>
      <th>purdue</th>
      <td>0.009551</td>
    </tr>
    <tr>
      <th>michigan-state</th>
      <td>0.009412</td>
    </tr>
    <tr>
      <th>gonzaga</th>
      <td>0.008738</td>
    </tr>
    <tr>
      <th>ohio-state</th>
      <td>0.008433</td>
    </tr>
    <tr>
      <th>cincinnati</th>
      <td>0.008282</td>
    </tr>
    <tr>
      <th>texas-christian</th>
      <td>0.008100</td>
    </tr>
  </tbody>
</table>
</div>


The above table includes the PageRank of the 25 most highly rated teams. It's notable that PageRank rated Villanova, the eventual winner of the 2018 NCAA tournament, as the best team in college basketball. According to the stationary distribution \\(\\boldsymbol\\pi\\), a random fan has a 2.4% chance of rooting for Villanova -- quite a high percentage when one considers that the ranking includes 656 teams.

To evaluate PageRank, I'll first compare its predictive accuracy in tournament games to rankings from Bleacher Report, 538, and the official NCAA list used to seed the tournament. These rankings include all 68 teams and were compiled shortly before the start of the tournament. The data has been cleaned and stored in a .csv file.


```python
# Load rankings.
file = 'ranking.csv'
ranking_df = pd.read_csv(path + file, index_col=0)

# Add PageRank ranking for tournament teams.
tourn_teams = ranking_df.index.tolist()
pr_dict = pr_df.to_dict()['PageRank']
pr = [ pr_dict[team] for team in tourn_teams ]
tuples = zip(tourn_teams, pr)
tuples = sorted(tuples, key=lambda tuple: tuple[1], reverse=True)
pr_teams, _ = zip(*tuples)
pr_dict = dict(zip(pr_teams, range(1, 70)))
ranking_df['PageRank'] = pd.Series(pr_dict)
with pd.option_context('display.max_rows', None):
    display(ranking_df)
```


<div align="middle" class="w3-responsive" style="overflow:auto; height:400px;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>BR</th>
      <th>538</th>
      <th>Seed</th>
      <th>PageRank</th>
    </tr>
    <tr>
      <th>Team</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>alabama</th>
      <td>23</td>
      <td>35</td>
      <td>36</td>
      <td>7</td>
    </tr>
    <tr>
      <th>arizona</th>
      <td>8</td>
      <td>15</td>
      <td>16</td>
      <td>32</td>
    </tr>
    <tr>
      <th>arizona-state</th>
      <td>36</td>
      <td>45</td>
      <td>43</td>
      <td>26</td>
    </tr>
    <tr>
      <th>arkansas</th>
      <td>30</td>
      <td>31</td>
      <td>26</td>
      <td>30</td>
    </tr>
    <tr>
      <th>auburn</th>
      <td>25</td>
      <td>25</td>
      <td>13</td>
      <td>17</td>
    </tr>
    <tr>
      <th>bucknell</th>
      <td>55</td>
      <td>51</td>
      <td>55</td>
      <td>60</td>
    </tr>
    <tr>
      <th>buffalo</th>
      <td>53</td>
      <td>48</td>
      <td>51</td>
      <td>54</td>
    </tr>
    <tr>
      <th>butler</th>
      <td>45</td>
      <td>23</td>
      <td>33</td>
      <td>5</td>
    </tr>
    <tr>
      <th>cal-state-fullerton</th>
      <td>63</td>
      <td>63</td>
      <td>61</td>
      <td>64</td>
    </tr>
    <tr>
      <th>cincinnati</th>
      <td>14</td>
      <td>6</td>
      <td>8</td>
      <td>24</td>
    </tr>
    <tr>
      <th>clemson</th>
      <td>19</td>
      <td>24</td>
      <td>19</td>
      <td>35</td>
    </tr>
    <tr>
      <th>college-of-charleston</th>
      <td>56</td>
      <td>55</td>
      <td>53</td>
      <td>57</td>
    </tr>
    <tr>
      <th>creighton</th>
      <td>37</td>
      <td>30</td>
      <td>30</td>
      <td>12</td>
    </tr>
    <tr>
      <th>davidson</th>
      <td>47</td>
      <td>37</td>
      <td>48</td>
      <td>50</td>
    </tr>
    <tr>
      <th>duke</th>
      <td>3</td>
      <td>3</td>
      <td>6</td>
      <td>11</td>
    </tr>
    <tr>
      <th>florida</th>
      <td>20</td>
      <td>18</td>
      <td>21</td>
      <td>9</td>
    </tr>
    <tr>
      <th>florida-state</th>
      <td>50</td>
      <td>27</td>
      <td>38</td>
      <td>41</td>
    </tr>
    <tr>
      <th>georgia-state</th>
      <td>58</td>
      <td>56</td>
      <td>60</td>
      <td>52</td>
    </tr>
    <tr>
      <th>gonzaga</th>
      <td>12</td>
      <td>9</td>
      <td>15</td>
      <td>22</td>
    </tr>
    <tr>
      <th>houston</th>
      <td>17</td>
      <td>19</td>
      <td>23</td>
      <td>18</td>
    </tr>
    <tr>
      <th>iona</th>
      <td>61</td>
      <td>59</td>
      <td>62</td>
      <td>65</td>
    </tr>
    <tr>
      <th>kansas</th>
      <td>5</td>
      <td>7</td>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>kansas-state</th>
      <td>33</td>
      <td>42</td>
      <td>34</td>
      <td>39</td>
    </tr>
    <tr>
      <th>kentucky</th>
      <td>9</td>
      <td>13</td>
      <td>17</td>
      <td>16</td>
    </tr>
    <tr>
      <th>lipscomb</th>
      <td>65</td>
      <td>62</td>
      <td>59</td>
      <td>58</td>
    </tr>
    <tr>
      <th>long-island-university</th>
      <td>67</td>
      <td>67</td>
      <td>66</td>
      <td>67</td>
    </tr>
    <tr>
      <th>loyola-il</th>
      <td>48</td>
      <td>43</td>
      <td>46</td>
      <td>42</td>
    </tr>
    <tr>
      <th>marshall</th>
      <td>43</td>
      <td>57</td>
      <td>54</td>
      <td>47</td>
    </tr>
    <tr>
      <th>maryland-baltimore-county</th>
      <td>62</td>
      <td>64</td>
      <td>63</td>
      <td>62</td>
    </tr>
    <tr>
      <th>miami-fl</th>
      <td>27</td>
      <td>26</td>
      <td>22</td>
      <td>45</td>
    </tr>
    <tr>
      <th>michigan</th>
      <td>10</td>
      <td>12</td>
      <td>11</td>
      <td>10</td>
    </tr>
    <tr>
      <th>michigan-state</th>
      <td>4</td>
      <td>4</td>
      <td>9</td>
      <td>21</td>
    </tr>
    <tr>
      <th>missouri</th>
      <td>21</td>
      <td>50</td>
      <td>32</td>
      <td>38</td>
    </tr>
    <tr>
      <th>montana</th>
      <td>54</td>
      <td>54</td>
      <td>56</td>
      <td>55</td>
    </tr>
    <tr>
      <th>murray-state</th>
      <td>49</td>
      <td>49</td>
      <td>50</td>
      <td>51</td>
    </tr>
    <tr>
      <th>nevada</th>
      <td>39</td>
      <td>39</td>
      <td>27</td>
      <td>36</td>
    </tr>
    <tr>
      <th>new-mexico-state</th>
      <td>44</td>
      <td>47</td>
      <td>47</td>
      <td>49</td>
    </tr>
    <tr>
      <th>north-carolina</th>
      <td>7</td>
      <td>8</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>north-carolina-central</th>
      <td>68</td>
      <td>68</td>
      <td>67</td>
      <td>66</td>
    </tr>
    <tr>
      <th>north-carolina-greensboro</th>
      <td>57</td>
      <td>53</td>
      <td>52</td>
      <td>48</td>
    </tr>
    <tr>
      <th>north-carolina-state</th>
      <td>34</td>
      <td>40</td>
      <td>37</td>
      <td>29</td>
    </tr>
    <tr>
      <th>ohio-state</th>
      <td>29</td>
      <td>20</td>
      <td>20</td>
      <td>23</td>
    </tr>
    <tr>
      <th>oklahoma</th>
      <td>40</td>
      <td>36</td>
      <td>40</td>
      <td>28</td>
    </tr>
    <tr>
      <th>pennsylvania</th>
      <td>60</td>
      <td>58</td>
      <td>64</td>
      <td>63</td>
    </tr>
    <tr>
      <th>providence</th>
      <td>28</td>
      <td>44</td>
      <td>35</td>
      <td>14</td>
    </tr>
    <tr>
      <th>purdue</th>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>20</td>
    </tr>
    <tr>
      <th>radford</th>
      <td>66</td>
      <td>65</td>
      <td>65</td>
      <td>59</td>
    </tr>
    <tr>
      <th>rhode-island</th>
      <td>24</td>
      <td>33</td>
      <td>28</td>
      <td>43</td>
    </tr>
    <tr>
      <th>san-diego-state</th>
      <td>38</td>
      <td>41</td>
      <td>45</td>
      <td>33</td>
    </tr>
    <tr>
      <th>seton-hall</th>
      <td>22</td>
      <td>21</td>
      <td>29</td>
      <td>37</td>
    </tr>
    <tr>
      <th>south-dakota-state</th>
      <td>42</td>
      <td>52</td>
      <td>49</td>
      <td>44</td>
    </tr>
    <tr>
      <th>st-bonaventure</th>
      <td>32</td>
      <td>46</td>
      <td>42</td>
      <td>53</td>
    </tr>
    <tr>
      <th>stephen-f-austin</th>
      <td>52</td>
      <td>60</td>
      <td>58</td>
      <td>56</td>
    </tr>
    <tr>
      <th>syracuse</th>
      <td>51</td>
      <td>38</td>
      <td>44</td>
      <td>46</td>
    </tr>
    <tr>
      <th>tennessee</th>
      <td>13</td>
      <td>17</td>
      <td>10</td>
      <td>19</td>
    </tr>
    <tr>
      <th>texas</th>
      <td>46</td>
      <td>34</td>
      <td>39</td>
      <td>31</td>
    </tr>
    <tr>
      <th>texas-am</th>
      <td>31</td>
      <td>28</td>
      <td>25</td>
      <td>13</td>
    </tr>
    <tr>
      <th>texas-christian</th>
      <td>41</td>
      <td>22</td>
      <td>24</td>
      <td>25</td>
    </tr>
    <tr>
      <th>texas-southern</th>
      <td>64</td>
      <td>66</td>
      <td>68</td>
      <td>68</td>
    </tr>
    <tr>
      <th>texas-tech</th>
      <td>16</td>
      <td>16</td>
      <td>12</td>
      <td>15</td>
    </tr>
    <tr>
      <th>ucla</th>
      <td>26</td>
      <td>32</td>
      <td>41</td>
      <td>40</td>
    </tr>
    <tr>
      <th>villanova</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>virginia</th>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>virginia-tech</th>
      <td>35</td>
      <td>29</td>
      <td>31</td>
      <td>34</td>
    </tr>
    <tr>
      <th>west-virginia</th>
      <td>15</td>
      <td>11</td>
      <td>18</td>
      <td>2</td>
    </tr>
    <tr>
      <th>wichita-state</th>
      <td>18</td>
      <td>14</td>
      <td>14</td>
      <td>27</td>
    </tr>
    <tr>
      <th>wright-state</th>
      <td>59</td>
      <td>61</td>
      <td>57</td>
      <td>61</td>
    </tr>
    <tr>
      <th>xavier</th>
      <td>6</td>
      <td>10</td>
      <td>4</td>
      <td>8</td>
    </tr>
  </tbody>
</table>
</div>


The table above shows all four rankings of the 68 tournament teams. In addition to the above rankings, I also found archived decimal odds for each tournament game from [Odds Portal](https://www.oddsportal.com/ "Odds Portal"). While these odds do not represent rankings _per se_, they nonetheless provide a meaningful benchmark of predictive accuracy.


```python
# Load and display odds.
file = 'odds.csv'
odds_df = pd.read_csv(path + file, index_col=0)
tourn_df = pd.merge(tourn_df, odds_df, on=['Team1', 'Team2'])
display(HTML(tourn_df.head().to_html(index=False)))
```

<div align="middle" class="w3-responsive">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Date</th>
      <th>Team1</th>
      <th>Pts1</th>
      <th>Team2</th>
      <th>Pts2</th>
      <th>Odds1</th>
      <th>Odds2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2018-03-13</td>
      <td>long-island-university</td>
      <td>61</td>
      <td>radford</td>
      <td>71</td>
      <td>3.01</td>
      <td>1.41</td>
    </tr>
    <tr>
      <td>2018-03-13</td>
      <td>st-bonaventure</td>
      <td>65</td>
      <td>ucla</td>
      <td>58</td>
      <td>2.27</td>
      <td>1.67</td>
    </tr>
    <tr>
      <td>2018-03-14</td>
      <td>arizona-state</td>
      <td>56</td>
      <td>syracuse</td>
      <td>60</td>
      <td>1.85</td>
      <td>1.99</td>
    </tr>
    <tr>
      <td>2018-03-14</td>
      <td>north-carolina-central</td>
      <td>46</td>
      <td>texas-southern</td>
      <td>64</td>
      <td>2.93</td>
      <td>1.43</td>
    </tr>
    <tr>
      <td>2018-03-15</td>
      <td>michigan</td>
      <td>61</td>
      <td>montana</td>
      <td>47</td>
      <td>1.16</td>
      <td>5.53</td>
    </tr>
  </tbody>
</table>
</div>

Decimal odds represent the return on a successful $1 bet. For example, in the last game in the above table, a successful $1 bet on Alabama would earn $2.28 (for a $1.28 profit) while a successful $1 bet on Virginia Tech would earn $1.66 (for a $0.66 profit). The team with the lower decimal odds (in this case, Virginia Tech) is considered the favorite.

I'll now compare rankings by counting the number of correctly predicted games, i.e., the number of games in which the higher ranked team won. I'll also count the number of tournament games correctly predicted by Vegas, i.e., the number of games in which the favorite won.


```python
# Index with True if team 1 won, False otherwise.
win_ind = tourn_df['Pts1'] > tourn_df['Pts2']

# Team lists.
cols = [ 'Team1', 'Team2' ]
teams1, teams2 = [ tourn_df[col].tolist() for col in cols ]

# Rankings.
data = list()
for col, rank_dict in ranking_df.to_dict().items():
    rank1 = np.array([ rank_dict[team] for team in teams1 ])
    rank2 = np.array([ rank_dict[team] for team in teams2 ])
    ind = rank1 < rank2
    data.append([col, sum(win_ind==ind)])

# Odds.
odds_ind = tourn_df['Odds1'] < tourn_df['Odds2']
data.append([ 'Vegas Odds', sum(win_ind==odds_ind) ])

columns = [ 'Ranking', 'Corr. Pred.' ]
comp_df = pd.DataFrame(data, columns=columns)
comp_df = comp_df.set_index('Ranking')
comp_df['Accuracy'] = comp_df['Corr. Pred.'] / 68
comp_df = comp_df.sort_values('Corr. Pred.', ascending=False)
display(comp_df)
```


<div align="middle" class="w3-responsive">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Corr. Pred.</th>
      <th>Accuracy</th>
    </tr>
    <tr>
      <th>Ranking</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>538</th>
      <td>47</td>
      <td>0.691176</td>
    </tr>
    <tr>
      <th>BR</th>
      <td>46</td>
      <td>0.676471</td>
    </tr>
    <tr>
      <th>Vegas Odds</th>
      <td>45</td>
      <td>0.661765</td>
    </tr>
    <tr>
      <th>Seed</th>
      <td>44</td>
      <td>0.647059</td>
    </tr>
    <tr>
      <th>PageRank</th>
      <td>43</td>
      <td>0.632353</td>
    </tr>
  </tbody>
</table>
</div>

A strategy of random guessing would result in an expected value of \\(\\mu = 33.5\\) correct predictions with standard deviation \\(\\sigma \\approx 4.09\\). The observed number of 43 correct predictions lies roughly 2.3 standard deviations above the mean and is statistically significant at the 0.05 level.

From the above table, we see that PageRank unperforms the other rankings, but by a narrow margin, falling short of the official seed ranking by a single game. This feat is perhaps more impressive when one considers that PageRank uses only a single game statistic, margin of victory, to produce its ranking. In contrast, the most accurate ranking from 538 employs a sophisticated ensemble of models.
