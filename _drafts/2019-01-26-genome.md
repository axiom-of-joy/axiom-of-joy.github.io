---
layout: post
title: "1000 Genomes Part I"
subtitle: "Unsupervised learning, dimensionality reduction, and visualization of genomic data."
author: "Alexander Song"
background: '/media/posts/genome/dna-3539309.jpg'
extra_css: '/assets/post_css/pagerank.css'
---


# 1000 Genomes Part I

The 1000 Genomes Project was an international effort commencing in 2008 and completed in 2015 to catalogue human genetic variation. The project sequenced the genomes of over 2500 individuals from 26 different ethnic groups, and the resulting data set was made publicly available for the benefit of future research. Each individual's genome was sampled at roughly 81 million sites (FIXME), resulting in a high-dimensional feature space. In this post, I use linear and non-linear dimensionality reduction techniques (namely, dual and kernel PCA) to visualize the data in lower dimensions. In spite of the high dimension of genomic data, it is possible to discern important structures in its lower dimensional representations, with distinct genders, ethnic, and racial groups appearing as discernible clusters. After visualizing the data, I use SVMs to predict the ethnic and racial origin of individuals based on their genome. This method correctly classifies ethnic origin into 26 classes with FIXME accuracy and racial origin into 5 classes with FIXME accuracy.

## Data Set

As previously mentioned, the 1000 Genomes data set consists of 2504 genomes sequenced at roughly 81 million sites. Since different individuals share the vast majority of their genomes in common, genomic data is stored in so-called "variant call format" (VCF), which records the differences between each individual genome and a reference genome. To illustrate, here's a small excerpt consisting of two rows from a VCF file from the 1000 Genomes Project (for clarity, I have omitted most columns and have formatted the text so that columns are properly aligned).

```
#CHROM  POS     REF     ALT     HG00096 HG00097 HG00099 HG00100 HG00101
1       10177   A       AC      1|0     0|1     0|1     1|0     0|0
```

The second row above corresponds to a gene located in the first chromosome at position 10177. The reference genome has the nucleic acid adenine at this site, while certain individuals have the alternate adenine-cytosine gene. Codes uniquely identify each individual in the study (e.g., "HG00096"). Below these ID codes, the digits to the left and right of the pipe signify whether the reference or alternate gene are present on the left and right alleles of the corresponding sample, with zero corresponding to the reference gene and one corresponding to the alternate gene. For example, sample HG00096 has the variant gene AC on the left allele and the reference gene A on the right allele at position 10177 of the first chromosome. In certain cases (not shown above), multiple alternate genes may be given, in which case positive digits indicate which variant is present. This format applies for chromosomes 1 through 21 and the X chromosome. Since human beings possess at most one allele for the Y chromosome, the VCF entries for this chromosome consist of single digits rather than digits separated by a pipe. In rare cases, a period indicates that data is missing for a particular individual and site.

### Processing the Data Set


By recording the sites at which an individual genome differs from the reference genome, variant call format takes advantage of the highly redundant nature of the human genome and lends itself naturally to a sparse matrix representation of the data. For each chromosome, I downloaded the corresponding VCF file from the 1000 Genomes S3 Bucket to an EC2 instance. I then parsed the file with a simple C program that iterates over positions in the genome and individual samples. At each position and for each individual, I ignored which particular variant occurred, instead recording only whether a variant occurred at all. For example, the data for first chromosome is stored in FIXME sparse matrices. Each matrix has 2504 rows corresponding to the samples in the data set, and the combined number of columns is just the number of studied sites in the first chromosome. The _(i, j)<sup>th</sup>_ entry of a matrix is 0 if the _i<sup>th</sup>_ sample matches the reference genome (on both alleles) at the site corresponding to the _j<sup>th</sup>_ column, and 1 if a variant occurs. This format is relatively space efficient, since the sparse matrices have Boolean rather than integer entries. Even so, the entire genomes of all 2504 individuals required roughly eight gigabytes of compressed sparse matrices.

Each of the supervised and unsupervised learning techniques I apply in this post and the next may be implemented using a pairwise distance matrix, so I elected to compute this matrix first. Since the data is binary, Manhattan distance is the same as squared Euclidean distance, i.e., \\(\lVert \mathbf x - \mathbf y \rVert_1 = \lVert \mathbf x - \mathbf y \rVert_2^2\\) for all samples \\(\mathbf x\\) and \\(\mathbf y\\). The Manhattan distance between two genomes has an attractive interpretation in the current context; it simply counts the number of sites at which one genome has the reference gene and the other genome has a variant. Moreover, since Manhattan distance and squared Euclidean distance are equal, we can efficiently compute the Manhattan pairwise distance matrix for each of the FIXME sparse matrices containing a portion of the data set using a vectorized implementation like the one found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html). Finally, the pairwise distance matrix for the entire data set can be easily computed by simply summing the pairwise distance matrices of each constituent sparse matrix.

## Dimensionality Reduction and Data Visualization

After obtaining the pairwise distance matrix in the manner described above, we can easily implement the linear and non-linear dimensionality reduction techniques of dual and kernel PCA.

### Dual PCA

Suppose a data set is represented by the matrix \\(X \in \mathbb R^{m \times n}\\), where \\(m\\) is the number of samples in the data set and \\(n\\) is the dimension of the feature space. To compute a standard PCA, we first center the data to obtain the matrix \\(X_c\\) with column means equal to zero. The principal components of the data are the right eigenvectors of the sample covariance matrix \\(\frac{1}{m-1}X_c^T X_c \in \mathbb R^{n \times n}\\), or equivalently, the right singular vectors of \\(X_c\\). If \\(V\\) denotes the matrix with \\(n\\) rows whose columns are principal components arranged in descending order by their corresponding eigenvalues, then a linear embedding of the data into \\(k < n\\) dimensions is given by \\(X_c V_k\\), where \\(V_k\\) is the matrix consisting of the top \\(k\\) principal components of \\(V\\).

In the current context, the number of features \\(n \approx 81\\) million far exceeds the number of samples \\(m = 2504\\). Due to the high dimension of the feature space, the techniques described above fail since we cannot explicitly compute the sample covariance matrix, nor the right singular vectors of \\(X_c\\) using a standard implementation of SVD. Since the number of samples \\(m = 2504\\) is relatively modest, a simple method known as _dual PCA_ may be used to compute the principal components. To motivate this method, note that the rank of \\(X_c^T X_c\\) is at most \\(\min\\{m, n\\} = m\\), so the sample covariance matrix actually has very low rank relative to its enormous size. Instead of constructing this massive matrix, we consider the so-called _Gram matrix_ \\(X_c X_c^T \in \mathbb R^{m \times m}\\), which is relatively small. You can show that \\(\mathbf v \in \mathbb R^m\\) is an eigenvector of \\(X_c X_c^T\\) corresponding to positive eigenvalue \\(\lambda\\) if and only \\(X_c^T \mathbf v \in \mathbb R^n\\) is an eigenvector of \\(X_c^T X_c\\) corresponding to the same eigenvalue. In other words, we can recover the principal components of the data from the eigenvectors of the Gram matrix \\(X_c X_c^T\\). Since the Gram matrix is small, these eigenvectors can be easily computed.

In a typical dual PCA, we would center the data \\(X\\) to obtain \\(X_c\\), compute the Gram matrix \\(X_c X_c^T\\) via matrix multiplication, and then compute the matrix \\(V\\) of right eigenvectors. If \\(V_k \in \mathbb R^{m \times k}\\) contains the top \\(k < m\\) eigenvectors in \\(V\\), then the top \\(k\\) principal components of the original data are \\(X_c^T V_k \in \mathbb R^{n \times k}\\), and an embedding of the data into \\(\mathbb R^k\\) is given by \\(X_c X_c^T V_k \in \mathbb R^{m \times k}\\). Unfortunately, even this procedure is infeasible in the current context, since explicit centering would destroy the sparse structure of the data. Luckily, I've already computed the pairwise squared Euclidean distance matrix \\(D_{\mathrm{sq}}\\) for \\(X\\). We can easily compute the Gram matrix with the formula
\\[
X_c X_c^T = -\left(I_m - \dfrac{\mathbf{1}\_m \mathbf{1}\_m^T}{m}\right) \frac{D_{\mathrm{sq}}}{2} \left(I_m - \dfrac{\mathbf{1}\_m \mathbf{1}\_m^T}{m}\right)
\\]
where \\(\mathbf{1}\_m \in \mathbb R^m\\) denotes the _one-vector_ whose entries are all one, and the outer product \\(\mathbf{1}\_m \mathbf{1}\_m^T\\) is the \\(m \times m\\) matrix whose entries are all one (see REF for a derivation). Now suppose the SVD of \\(X_c\\) is given by \\(X_c = U \Sigma V^T\\). Then \\(X_c X_c^T = U \Sigma^2 U^T\\), so we can obtain the left singular vectors of \\(X_c\\) by computing the left singular vectors of the Gram matrix. Thus, the coordinates of the projection of the data onto the top \\(k\\) principal components is given by
\\[
U_k \Sigma_k = X_c X_c^T V_k
\\]
where \\(U_k \in \mathbb R^{m \times k}\\) contains the top \\(k\\) left singular vectors of \\(X_c\\) and \\(\Sigma_k \in \mathbb R^{k \times k}\\) is the diagonal matrix with diagonal entries equal to the top \\(k\\) singular values. As explained in this [enlightening Stack Exchange post](https://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-component-analysis-and-multidimensional/132731#132731), this procedure to compute dual PCA is equivalent to classical multidimensional scaling using Euclidean distances.

While explanation I gave above is long-winded, the actual implementation is extremely simple.

<h3><a id="kernelPCA">Kernel PCA</a></h3>




<iframe src="https://www.axiom-of-joy.com/" frameBorder="0" width="100%"></iframe>




## References

Mathematics -- PCA in High Dimensions
https://www.youtube.com/watch?v=NhrhppL4suE


Stack Exchange
https://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-component-analysis-and-multidimensional/132731#132731
