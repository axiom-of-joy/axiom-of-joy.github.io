---
layout: post
title: "1000 Genomes Part I"
subtitle: "Unsupervised learning, dimensionality reduction, and visualization of genomic data."
author: "Alexander Song"
background: '/media/posts/genome/dna-3539309.jpg'
extra_css: '/assets/post_css/pagerank.css'
---


# 1000 Genomes Part I

The 1000 Genomes Project was an international effort commencing in 2008 and completed in 2015 to catalogue human genetic variation. The project sequenced the genomes of over 2500 individuals from 26 different ethnic groups, and the resulting data set was made publicly available for the benefit of future research. Each individual's genome was sampled at roughly 81 million sites (FIXME), resulting in a high-dimensional feature space. In this post, I use linear and non-linear dimensionality reduction techniques (namely, dual and kernel PCA) to visualize the data in lower dimensions. In spite of the high dimension of genomic data, it is possible to discern important structures in its lower dimensional representations, with distinct genders, ethnic, and racial groups appearing as discernible clusters. After visualizing the data, I use SVMs to predict the ethnic and racial origin of individuals based on their genome. This method correctly classifies ethnic origin into 26 classes with FIXME accuracy and racial origin into 5 classes with FIXME accuracy.

## Data Set

As previously mentioned, the 1000 Genomes data set consists of 2504 genomes sequenced at roughly 81 million sites. Since different individuals share the vast majority of their genomes in common, genomic data is stored in so-called "variant call format" (VCF), which records the differences between each individual genome and a reference genome. To illustrate, here's a small excerpt consisting of two rows from a VCF file from the 1000 Genomes Project (for clarity, I have omitted most columns and have formatted the text so that columns are properly aligned).

```
#CHROM  POS     REF     ALT     HG00096 HG00097 HG00099 HG00100 HG00101
1       10177   A       AC      1|0     0|1     0|1     1|0     0|0
```

The second row above corresponds to a gene located in the first chromosome at position 10177. The reference genome has the nucleic acid adenine at this site, while certain individuals have the alternate adenine-cytosine gene. Codes uniquely identify each individual in the study (e.g., "HG00096"). Below these ID codes, the digits to the left and right of the pipe signify whether the reference or alternate gene are present on the left and right alleles of the corresponding sample, with zero corresponding to the reference gene and one corresponding to the alternate gene. For example, sample HG00096 has the variant gene AC on the left allele and the reference gene A on the right allele at position 10177 of the first chromosome. In certain cases (not shown above), multiple alternate genes may be given, in which case positive digits indicate which variant is present. (The above format applies for chromosomes 1 through 21 and the X chromosome. Since human beings possess at most one allele for the Y chromosome, the VCF entries for this chromosome consist of single digits rather than digits separated by a pipe.) In rare cases, a period indicates that data is missing for a particular individual and site.

### Processing the Data Set


By recording the sites at which an individual genome differs from the reference genome, variant call format takes advantage of the highly redundant nature of the human genome and lends itself naturally to a sparse matrix representation of the data. For each chromosome, I downloaded the corresponding VCF file from the 1000 Genomes S3 Bucket to an EC2 instance. I then parsed the file with a simple C program that iterates over positions in the genome and individual samples. At each position and for each individual, I ignored which particular variant occurred, instead recording only whether a variant occurred at all. For example, the data for first chromosome is stored in FIXME sparse matrices. Each matrix has 2504 rows corresponding to the samples in the data set, and the combined number of columns is just the number of studied sites in the first chromosome. The _(i, j)<sup>th</sup>_ entry of a matrix is 0 if the _i<sup>th</sup>_ sample matches the reference genome (on both alleles) at the site corresponding to the _j<sup>th</sup>_ column, and 1 if a variant occurs. This format is relatively space efficient, since the sparse matrices have Boolean rather than integer entries. Even so, the entire genomes of all 2504 individuals required roughly eight gigabytes of compressed sparse matrices.

Each of the supervised and unsupervised learning techniques I apply in this post and the next may be implemented using a pairwise distance matrix, so I elected to compute this matrix first. Since the data is binary, Manhattan distance is the same as squared Euclidean distance, i.e., \\(\lVert \mathbf x - \mathbf y \rVert_1 = \lVert \mathbf x - \mathbf y \rVert_2^2\\) for all samples \\(\mathbf x\\) and \\(\mathbf y\\). The Manhattan distance between two genomes has an attractive interpretation in the current context; it simply counts the number of sites at which one genome has the reference gene and the other genome has a variant. Moreover, since Manhattan distance and squared Euclidean distance are equal, we can efficiently compute the Manhattan pairwise distance matrix for each of the FIXME sparse matrices containing a portion of the data set using a vectorized implementation like the one found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html). Finally, the pairwise distance matrix for the entire data set can be easily computed by simply summing the pairwise distance matrices of each constituent sparse matrix.

## Dimensionality Reduction and Data Visualization

After obtaining the pairwise distance matrix in the manner described above, we can easily implement the linear and non-linear dimensionality reduction techniques of dual and kernel PCA.

### Dual PCA

Suppose our data set is represented by the matrix \\(X \in \mathbb R^{m \times n}\\), where \\(m = 2504\\) is the number of samples in the data set and \\(n \approx 81\\) million is the dimension of the feature space. The standard technique to compute the principal components of the data uses the eigendecomposition of the sample covariance matrix \\(\frac{1}{m-1}X_c^T X_c \in \mathbb R^{n \times n}\\), where \\(X_c\\) is the result of centering the data \\(X\\) to have column means equal to zero. This approach is infeasible in the current context, since the high dimension of the feature space results in an impractically large covariance matrix. If the number of samples is relatively modest (as in the current case of \\(m = 2504\\)), a simple method known as _dual PCA_ may be used to compute the principal components. To motivate this method, note that the rank of \\(X_c^T X_c\\) is at most \\(\min\\{m, n\\} = m\\), so the sample covariance matrix has at most \\(m\\) non-zero eigenvalues (these eigenvalues are in fact positive since \\(X_c^T X_c\\) is positive semidefinite). So the sample covariance matrix actually has very low rank relative to its enormous size. Instead of constructing this massive matrix, let's consider the linear kernel \\(X_c X_c^T \in \mathbb R^{m \times m}\\), which is relatively small. You can show that \\(\mathbf v \in \mathbb R^m\\) is an eigenvector of \\(X_c X_c^T\\) corresponding to positive eigenvalue \\(\lambda\\) if and only \\(X_c^T \mathbf v \in \mathbb R^n\\) is an eigenvector of \\(X_c^T X_c\\) corresponding to the same eigenvalue. In other words, we can recover the principal components of the data from the eigenvectors of \\(X_c X_c^T\\), which may be easily computed.

In a typical dual PCA, we would center the data \\(X\\) to obtain \\(X_c\\) and then compute the linear kernel \\(X_c X_c^T\\) via matrix multiplication. This procedure is infeasible in the current context, since (a) explicit centering would destroy the sparse structure of the data, and (b) matrix multiplication with \\(X_c\\) is cumbersome due to the size of the data and the format in which it has been stored. Luckily, I've already computed the pairwise squared Euclidean distance matrix \\(D_{\mathrm{sq}}\\) for \\(X\\), since this matrix is needed for <a href="#kernelPCA">kernel PCA</a>. You can show that the linear kernel can be computed as
\\begin{equation}
X_c X_c^T = -\left(I_m - \dfrac{\mathbf{1}_m \mathbf{1}_m^T}{m}\right) \dfrac{D_{\mathrm{sq}}}{2}
\\end{equation}

\\(-\left(I_m - \dfrac{\mathbf{1}_m \mathbf{1}_m^T}{m}\right)\\)

where \\(\mathbf{1} \in \mathbb R^m\\) denotes the _one-vector_ whose entries are all one, and the outer product \\(\mathbf{1}_m \mathbf{1}_m^T\\) is the square \\(m \times m\\) matrix whose entries are all one.

 \\(X_c X_c^T\\) is just the linear kernel \\(K_c\\) for the centered data \\(X_c\\)

<h3><a id="kernelPCA">Kernel PCA</a></h3>


https://stats.stackexchange.com/a/132731/233704

<iframe src="https://www.axiom-of-joy.com/" frameBorder="0" width="100%"></iframe>




## References

Mathematics -- PCA in High Dimensions
https://www.youtube.com/watch?v=NhrhppL4suE


Stack Exchange
https://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-component-analysis-and-multidimensional/132731#132731
